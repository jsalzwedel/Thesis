\subsection{Systematic uncertainties of correlation functions}
\label{sec:SysUncertaintyCF}

We will now attempt to identify the various systematic effects that may affect the correlation functions.  
The goal will be to quantify the systematic uncertainties associated with each $k^*$ bin.  
It should be noted, however, that subsequent fits to the correlation functions are not directly impacted by these error bars.  
Instead, the systematics on the resulting fit parameters will be computed by refitting many versions of the correlation functions (e.g.\ those built with different reconstruction cuts) and comparing the results. 
This will be discussed in greater detail in Section \ref{sec:SysErrorsFitsCuts}.

\subsubsection{Consistency checks for uncorrelated histograms}
\label{sec:ConsistencyCheckUncorrelated}
One of the tools we will employ in the search for systematic effects is the TH1::Chi2Test(TH1* h1) method, which compares two histograms bin-by-bin to determine a $\chi^2$.  
The test is performed assuming the hypothesis that the two histograms are Poissonian samples of the same underlying distribution.  
A p-value is computed which represents the probability that a measurement could be performed that would yield results this different or more, given the hypothesis of identity.  
The hypothesis of identity is rejected in instances where the p-value is lower than 0.01. 
At that threshold, less than one percent of tests will fail of data sets sampled from the same underlying distribution.  
This corresponds to rejecting the hypothesis of identity for data samples that differ by roughly $3 \sigma$.  
The 0.01 metric is somewhat arbitrary - it was chosen to be small enough that pure statistical fluctuations should not often lead to false or unnecessary systematic errors.  
For example, if a p-value of 0.1 were used instead, this would result in roughly 10\% of "good" results failing the test and contributing to the uncertainty, which would lead to a gross overestimation of the total systematic error.  
Similarly, a threshold of 0.001 would probably underestimate the error.  
Thus, 0.01 was chosen.

In this analysis, Chi2Test() will be used to compare two correlation functions which are uncorrelated with each other (uncorrelated in the sense that they are completed independent data samples).  
One example where this can be employed is the comparison of correlation functions constructed using data taken under different field configurations.  
This analysis needs all the data available to it, so results taken under different field configurations will need to be merged to improve statistics.  
But before they are combined, it must be checked that they contain compatible results.  
The Chi2Test() method provides a means of characterizing the degree of similarity of the correlation functions.  

Correlation functions that surpass the significance level of 0.01 will be treated as being functionally the same within statistics, and their weighted average will be computed without recourse to systematic uncertainty. 
In the instances where they do not exceed the significance level, they will be analyzed on a case-by-case basis.  
In particular, we will look to see if there are any systematic differences between the correlation functions.  
For example, it may be seen that the lowest ten $k^*$ bins will all be higher in one correlation function than another.  
If so, the data will still be merged (so long as the results aren't dramatically different), but a systematic uncertainty will be computed and applied to the final averaged correlation function in the form of systematic error bars.  
Section \ref{sec:CalculatingSysErrors} describes how these uncertainties are calculated.  

When using Chi2Test() to compare correlation functions, the trouble areas that result in small p-values will likely be the lowest $k^*$ bins (which exhibit most of the interesting physics, and which also suffer the most from two-track effects), and large $k^*$ bins (close to $1 \mathrm{GeV/c}$).  
The height of the the large $k^*$ bins is noticeably susceptible to statistical (or systematic) deviations in the normalization region.  
The high $k^*$ bins have relatively small statistical error bars, so small deviations of normalization between correlation functions can result in large $\chi^2$ values.  
The correlation functions are all normalized to unity in the $0.3 < k^* < 0.5 \mathrm{GeV}/c$ range, so there are unlikely to be significant deviations seen in that range.  

It is important to emphasize that Chi2Test() is only appropriate for comparing two uncorrelated histograms.  
In the case of correlated histograms (e.g.\ correlation functions constructed using slightly different two-track cuts or different reconstruction cuts), a different method must be used to test the histograms for consistency.  


\subsubsection{Consistency checks for correlated histograms}
\label{sec:ConsistencyCheckCorrelated}
Chi2Test() cannot be used to compare correlated data because the test is performed assuming the statistical error bars on the two histograms are independent.  
That is obviously not the case for two histograms that differ in their data only be a few percent.  
In this analysis, we perform consistency checks of correlated data in the following fashion:

\begin{enumerate}
\item Take the difference between the two correlation function histograms $\Delta C(k^*) = C_1(k^*) - C_2(k^*)$, using $C_1\rightarrow$ Add($C_2$,$-1$).  
The resulting histogram shows the usually small differences between the two data sets.
\item Here, ROOT incorrectly adds the errors in quadrature.  
To fix this, manually set the error bars using $\sigma_{\Delta}(k^*) = \sqrt{ \abs{ \sigma_1^2(k^*) - \sigma_2^2(k^*) }}$.  
This error is correct in the specific case (found here) that one histogram is entirely a subset of the other.
\item We now look to see if there are any significant discrepancies or systematic differences between the two original correlation functions.
If there is a structural difference, it will generally be largest in the lowest $k^*$ bins, dropping off as $k^*$ increases.
This shape is reasonably described by a decaying exponential: $f(k^*) = a \exp(-b k^*)$. 
The amplitude $a$ tells us the size of the discrepancy. The width $b$ has no significance; it just helps the fit.
\item We fit the histogram with the exponential, and we extract the amplitude $a$ and its error $\sigma_a$.
For this analysis, we conclude that there is no discrepancy if $a$ is within 2$\sigma$ of zero.
If $a$ is more than 2$\sigma$ from zero, then we have a source of systematic uncertainty.
\item In the cases where there is a systematic deviation, we take the height of the fit function in the center of each $k^*$ bin as the systematic uncertainty for that bin from this cut.
As the fit function can be positive or negative, this leads to asymmetric errors.

\end{enumerate}

Using the steps listed above, we can evaluate if seemingly small changes to the cuts employed in correlation function construction change the resulting data in any significant way.  


\subsubsection{Systematic errors from reconstruction cuts}
\label{sec:SystematicsReconstruction}

In this section we discuss the systematic errors associated with the V0 reconstruction cuts (e.g.\ cosine of pointing angle, DCA to primary vertex, etc.).  
Loosely speaking, these systematic checks test the sensativity of the correlation function to small changes in the $\Lambda$ purity.
The optimal cut values were discussed in Section \ref{sec:Recon}.  
For example, the optimal cut value chosen for the DCA of daughter tracks to each other was 4 mm (daughters tracks were required to pass within that distance of each other).    
However, there is some ambiguity in determining the optimal cut value.  
For example, any value within $4\pm10\%$ would be reasonable.
The correlation function should ideally be insensitive to small changes in the cut value.
If a small tweak to the cut values produces a statistically significant change in the correlation function, that is a source of systematic uncertainty.

To test for systematic differences, we make correlation functions using $\Lambda$ ($\bar{\Lambda}$) reconstructed with these different cut values. 
At any given time, one cut type (e.g.\ DCA of proton daughter to primary vertex) will be varied away from the default cut value by $\pm10\%$, while the other cut types (e.g.\ cosine of pointing angle) are fixed to their default values. 
  
Data has been collected for variations of the following cuts: 
\begin{itemize}
\item DCA of proton daughter to primary vertex
\item DCA of pion daughter to primary vertex
\item DCA of daughters to each other
\item Proper decay length of $\Lambda$
\item Eta of $\Lambda$
\item Cosine of $\Lambda$ pointing angle
\item DCA of the $\Lambda$ to primary vertex
\item $p_{\mathrm{T}}$ of $\Lambda$
\item Reconstructed mass ($m_{\pi\mathrm{p}}$)
\end{itemize}
  
In each case, correlation functions have been constructed for each pair type ($\Lambda\Lambda$, $\bar{\Lambda}\bar{\Lambda}$, and $\Lambda\bar{\Lambda}$), and for the three cut values (optimal cut, cut + 10\%, cut - 10\%).  
The optimal cut correlation functions are compared with the tighter and looser cut correlation functions.  
The correlation functions all share much of the same data, so they are highly correlated with each other.  
We therefore analyze the changes in the correlation function using the method described in Section \ref{sec:ConsistencyCheckCorrelated}.

Checking each cut yields a positive, negative, or non-existent systematic uncertainty from that cut for each $k^*$ bin. 
For a given bin, we take the positive and negative values with the largest magnitude to be the upper and lower asymmetric errors, respectively.


\subsubsection{Systematic errors from pair-wise cuts}
\label{sec:SystematicsPairWise}

The systematic uncertainties from the average separation cut are implemented in exactly the same way as the single-particle reconstruction cuts in the previous section. 
For each cut value, we vary the cut by $\pm10\%$, construct correlation functions, and find the uncertainty for each $k^*$ bin via the method in Section \ref{sec:ConsistencyCheckCorrelated}.
In general, the systematic uncertainties are quite small --- about an order of magnitude smaller than the uncertainties from the reconstruction cuts.

\subsubsection{Systematic errors from shared-daughter cut}

At this time, two femtoscopic analyses at ALICE employ a shared-daughter cut (this analysis and the $\mathrm{K}^0_\mathrm{S}$ analysis), though each cut is implemented with independent code.  
The reasons for this cut are intuitive - two V0s cannot actually have the same daughter, regardless of what might arise from the reconstruction process.  
Moreover, the efficacy of the cut is high: the MC studies of this cut have shown that only 13\% real V0s with shared daughters are cut.  
The removal of a V0 (real or fake) removes all of the same-event and mixed-event pairs that would have included that V0.  
In most cases, this amounts to an improved purity of the correlation function, since there are fewer pairs of fake and therefore uncorrelated particles.  
There also a removal of a splitting-like effect - in this case the "split" is an extra V0 close in momentum space to the first.  
While the two "split" V0s will never be paired together, both are paired with other V0s, resulting in extra pair counts in roughly the same $k^*$ bins.  
Those extra pairs are removed by virtue of the shared daughter cut.

While the benefits of the shared-daughter cut are apparent,
and the disadvantages seem to be minimal (even when you cut a real V0, you a left with a fake V0 that is close in phase space), it is unclear at this time how to determine what systematic uncertainties may arise from this cut and how to quantify that uncertainty.

%\subsubsection{Systematic errors from momentum resolution correction}
%\label{sec:MomentumResCorrectionsSys}
%
%As discussed in Section \ref{sec:MomentumResCorrection}, sources of systematic uncertainty in this correction come from using $p_\mathrm{T}$- and centrality-integrated measurements of the relative momentum smearing.  
%One correction is applied to all centrality bins, and that correction is made assuming a source radius of $2.5$ fm. 
%Generally speaking, the effect of momentum smearing will be larger the larger the various correlation function parameters are.  
%Those parameters include the source radii, the $\lambda$ (pair-fraction) parameter, the scattering lengths, and the effective range of interaction.  
%Depending on the centrality, early fits to the radii suggest that values between $2.2$ fm and $2.7$ fm could be reasonable.  
%The choice of the $\lambda$ parameter strongly affects the magnitude of the correction.  
%With a $\lambda$ of 0.3, the correction is about $3\%$ for $\Lambda\Lambda$.  
%But with an (unreasonable) $\lambda$ of 1, the correction is closer to $20\%$.  
%The choice of $\lambda$ parameter is complicated by the existence of secondary $\Lambda$ and residual correlations (see Section \ref{sec:Residual}).  
%For this analysis, $\lambda = 0.3$ was chosen such that the simulated, momentum-smeared correlation function roughly matched the data with a y-intercept of $\sim 0.9$.
%
%A more precise measurement of the necessary correction could be done (and will eventually be done) more directly with the HIJING MC data.  
%Instead of extracting a roughly Gaussian relative momentum smearing width from Figure \ref{fig:MomSmearingFit} and crudely simulating the $k^*_{\mathrm{true}} \rightarrow k^*_{\mathrm{recon}}$ smearing, the correlation functions $C_{\mathrm{true}}$ and  $C_{\mathrm{true}}$ (Eqs.\ \ref{eq:Ctrue} and \ref{eq:Crecon}) can be constructed directly from analysis of the MC data.  
%There, the correlation functions are binned using the known $k^*_{\mathrm{true}}$ and $k^*_{\mathrm{recon}}$.  
%The same weight factors are used as described in Section \ref{sec:MomentumResCorrection}.  
%The disadvantage of that method is that it requires assumptions of the various correlation function parameters at run-time, and so it requires significantly more analysis grid time as the parameters are tweaked.  
%However, that method is also considered to give more precise results.
%
%The $K^0_\mathrm{S}K^0_\mathrm{S}$ femtoscopic analysis has explored both of these methods.  
%In that analysis, it found that the method employed here (extracting a simple Gaussian smearing width and using it to simulate the momentum smearing) overestimated the momentum correction factor by up to a factor of 4 relative to the more precise method (i.e. a 4\% effect rather than a 1\% effect).  
%The similarity of the $K^0_\mathrm{S}K^0_\mathrm{S}$ analysis and the $\Lambda\Lambda$ analysis (both study V0 femtoscopy) suggests that the momentum resolution correction factor is overestimated here as well.
%
%Given the issues listed above, it is possible for the resolution correction made here to be over or underestimated. 
%At this point in the analysis, we will make a rough estimate of a symmetric systematic uncertainty for each correlation function, calculated using
%\begin{equation}
%\sigma_{\mathrm{sys}}(k^*) = \frac{1}{4}\abs{C_{\mathrm{uncorrected}} (k^*) - C_{\mathrm{corrected}}(k^*)}.
%\end{equation}
%The corrections are performed on centrality merged (and pair type merged, in the case of the $\Lambda\Lambda + \bar{\Lambda}\bar{\Lambda}$ data) correlation functions.  
%The uncertainty is taken then from the difference between the corrected and uncorrected versions of these merged plots.

\subsubsection{Combining different sources of systematic error}
\label{sec:CombiningSys}

Systematic errors have been evaluated for each type of cut, each pair type, and each centrality bin.  
For a given centrality bin and pair type, the histograms containing those errors have been added in quadrature for each $k^*$ bin.  
The resulting summed histogram is the systematic uncertainty for that centrality bin and pair type.  
Tables \ref{tab:SysErrorSourcesLL} - \ref{tab:SysErrorSourcesLA} are provided to give ballpark estimates of the systematic errors from each source.  
The tables show the average (over each $k^*$ bin) error size for a given uncertainty source, as well as the largest value of error found in any $k^*$ bin.

When correlation functions for multiple centrality bins are merged together, the errors for each centrality bin are added in quadrature (for each $k^*$ bin) and then averaged over the number of centrality bins.  
Similarly, when combining the $\Lambda\Lambda$ and $\bar{\Lambda}\bar{\Lambda}$ results, the systematic errors for each centrality range are added in quadrature and then divided by two.  
Subsequently, the systematic errors associated with the momentum resolution correction of each correlation function is added in quadrature with the rest of the errors.

The resulting correlation functions with combined systematic errors are shown in Section \ref{sec:CorrelationFunctions}.

\begin{table}
\caption[Systematic error contributions for $\Lambda\Lambda$] {Systematic error contributions for $\Lambda\Lambda$.  
If a particular centrality failed p-value tests for tighter and looser cuts of a particular cut type, it will have two sources of error for that cut type.} \label{tab:SysErrorSourcesLL} 
\begin{center}
\begin{tabular}{| c | c | c | c | c |}
  \hline                       
  Pair Type & Centrality Range & Error Source & Average Error & Max Error \\
  \hline
  $\Lambda\Lambda$ & 0-5\% & Avg Sep Prot & 1.8e-5 & 5.7e-5 \\
   &         & Avg Sep Pion & 2.1e-5 & 8.7e-5 \\
   &         & Avg Sep Prot & 1.1e-5 & 3.7e-5 \\
   &         & Avg Sep Pion & 1.3e-5 & 6.6e-5 \\
   & 5-10\%  & Avg Sep Prot & 9.4e-6 & 4.8e-5 \\
   &         & Avg Sep Pion & 8.6e-6 & 1.0e-5 \\
   & 10-15\% & $\cos(\Theta_{\mathrm{P}})$ & 4.4e-4 & 1.9e-3 \\
   &         & $B$ Fields & 5.2e-3 & 2.4e-2 \\
   &         & Avg Sep Prot & 3.0e-5 & 7.0e-5 \\
   &         & Avg Sep Pion & 2.9e-5 & 8.6e-5 \\
   &         & Avg Sep Pion & 4.9e-5 & 1.8e-4 \\
   & 15-20\% & Avg Sep Pion & 3.5e-5 &  1.2e-4 \\
   &         & Avg Sep Pion & 4.3e-5 & 1.5e-4 \\
   & 20-25\% & $\cos(\Theta_{\mathrm{P}})$ & 1.0e-3 & 9.6e-3 \\
   &         & Avg Sep Prot & 1.1e-4 & 7.1e-4 \\
   & 25-30\% & V0 DCA & 9.9e-4 & 1.9e-3\\
   &	         & Pion DCA & 1.4e-3 & 7.0e-3\\
   &         & Avg Sep Pion & 7.5e-5 & 4.6e-4 \\
   & 30-35\% & Pion DCA & 2.4e-3 & 2.7e-2 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 1.4e-3 & 1.5e-2 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 6.8e-4 & 2.9e-3 \\
   & 35-40\% & $\cos(\Theta_{\mathrm{P}})$ & 1.1e-3 & 7.3e-3 \\
   & 40-45\% & Pion DCA & 1.9e-3 & 6.3e-3 \\
   &         & Prot DCA & 9.6e-3 & 1.2e-1 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 1.4e-3 & 5.1e-3 \\ 
   & 45-50\% & V0 DCA & 3.7e-3 & 9.9e-3 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 3.9e-3 & 4.1e-2 \\
   \hline
\end{tabular}
\end{center}
\end{table}
\begin{table}
\caption[Systematic error contributions for $\bar{\Lambda}\bar{\Lambda}$] {Systematic error contributions for $\bar{\Lambda}\bar{\Lambda}$.  
If a particular centrality failed p-value tests for tighter and looser cuts of a particular cut type, it will have two sources of error for that cut type.} \label{tab:SysErrorSourcesAA} 
\begin{center}
\begin{tabular}{| c | c | c | c | c |}
  \hline                         
  Pair Type & Centrality Range & Error Source & Average Error & Max Error \\
  \hline 
  $\bar{\Lambda}\bar{\Lambda}$ & 0-5\% & Pion DCA & 5.4e-4 & 4.6e-3 \\
   &        & Daughter DCA & 7.7e-4 & 7.4e-3 \\
   &        & Avg Sep Prot & 2.1e-5 & 5.5e-5 \\
   &         & Avg Sep Prot & 1.8e-5 & 5.2e-5 \\
   & 5-10\% & Pion DCA & 2.0e-4 & 2.0e-3 \\
   &         & Avg Sep Prot & 1.0e-5 & 1.1e-5 \\
   &         & Avg Sep Pion & 8.4e-6 & 1.4e-5 \\
   & 10-15\% & Avg Sep Prot & 9.9e-6 & 3.8e-5 \\
   &         & Avg Sep Pion & 2.5e-5 & 8.6e-5 \\
   & 15-20\% & Avg Sep Prot & 1.7e-4 & 1.0e-3 \\
   &         & Avg Sep Pion & 4.1e-5 & 1.5e-4 \\
   &         & Avg Sep Pion & 6.0e-5 & 2.2e-4 \\
   & 20-25\% & Prot DCA & 2.6e-3 & 2.1e-2 \\
   &         & $B$ Fields & 9.1e-3 & 3.6e-2 \\
   & 25-30\% & $\cos(\Theta_{\mathrm{P}})$ & 5.1e-4 & 3.7e-3 \\
   & 30-35\% & Pion DCA & 1.6e-3 & 1.3e-2 \\
   &         & $B$ Fields & 1.4e-2 & 9.3e-2 \\
   &         & Avg Sep Prot & 8.3e-5 & 4.9e-4 \\
   & 35-40\% & V0 DCA & 2.8e-3 & 1.1e-2 \\
   &         & Pion DCA & 2.4e-4 & 2.7e-2 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 2.8e-3 & 2.1e-2 \\
   & 40-45\% & Pion DCA & 3.5e-3 & 3.4e-3 \\
   &         & Pion DCA & 3.3e-3 & 3.9e-2 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 1.2e-3 & 9.7e-3 \\
   & 45-50\% & V0 DCA & 1.1e-2 & 1.2e-1 \\
   &         & Pion DCA & 3.6e-3 & 3.2e-3 \\
   &         & Pion DCA & 6.7e-3 & 4.8e-2 \\
   &         & Daughte DCA & 4.0e-3 & 2.7e-2 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 2.7e-3 & 1.0e-2 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 1.8e-3 & 9.3e-3 \\
   \hline
\end{tabular}
\end{center}
\end{table}
\begin{table}
\caption[Systematic error contributions for $\Lambda\Lambda + \bar{\Lambda}\bar{\Lambda}$] {Systematic error contributions for $\Lambda\Lambda + \bar{\Lambda}\bar{\Lambda}$.  
If a particular centrality failed p-value tests for tighter and looser cuts of a particular cut type, it will have two sources of error for that cut type.} \label{tab:SysErrorSourcesLLAA} 
\begin{center}
\begin{tabular}{| c | c | c | c | c |}
  \hline                       
  Pair Type & Centrality Range & Error Source & Average Error & Max Error \\
  \hline
  $\Lambda\Lambda + \bar{\Lambda}\bar{\Lambda}$ & Momentum Res & 0-10\% & 2.9e-4 & 8.2e-3 \\
   & 10-30\% & Momentum Res & 2.6e-4 & 8.3e-3 \\
   & 30-50\% & Momentum Res & 2.7e-3 & 8.3e-3 \\
  \hline
\end{tabular}
\end{center}
\end{table}
\begin{table}
\caption[Systematic error contributions for $\Lambda\bar{\Lambda}$] {Systematic error contributions for $\Lambda\bar{\Lambda}$.  
If a particular centrality failed p-value tests for tighter and looser cuts of a particular cut type, it will have two sources of error for that cut type.} \label{tab:SysErrorSourcesLA} 
\begin{center}
\begin{tabular}{| c | c | c | c | c |}
  \hline                        
  Pair Type & Centrality Range & Error Source & Average Error & Max Error \\
  \hline  
  $\Lambda\bar{\Lambda}$ &  0-5\% & V0 DCA & 2.8e-4 & 2.2e-3 \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 1.6e-4 & 1.6e-3 \\
   &         & Avg Sep Prot & 2.6e-6 & 6.7e-6 \\
   & 5-10\%  & Avg Sep Prot & 6.4e-6 & 9.2e-6 \\
   & 10-15\% & Avg Sep Prot & 8.5e-6 & 3.7e-5 \\
   & 15-20\% & $B$ Field & 4.3e-3 & 1.9e-2 \\
   & 20-25\% & $\cos(\Theta_{\mathrm{P}})$ & 5.4e-4 & 5.2e-3 \\
   & 25-30\% & V0 DCA & 6.2e-4 & 3.2e-3 \\
   & 30-35\% & Nothing & &  \\
   & 35-40\% & Nothing & &  \\
   & 40-45\% & Pion DCA & 8.6e-4 & 2.1e-3  \\
   &         & $\cos(\Theta_{\mathrm{P}})$ & 1.9e-3 & 2.4e-2 \\
   & 45-50\% & Nothing & &  \\
   & 0-10\%  & Momentum Res & 1.6e-4 & 1.4e-3 \\
   & 10-30\% & Momentum Res & 1.1e-3 & 4.9e-3 \\
   & 30-50\% & Momentum Res & 6.0e-4 & 6.0e-3 \\
  \hline  
\end{tabular}
\end{center}
\end{table}

