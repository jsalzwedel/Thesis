\section{Systematic uncertainties of correlation functions}
\label{sec:SysUncertaintyCF}

We will now attempt to identify the various systematic effects that may affect the correlation functions.  
The goal will be to quantify the systematic uncertainties associated with each $k^*$ bin.  
It should be noted, however, that subsequent fits to the correlation functions are not directly impacted by these error bars.  
Instead, the systematics on the resulting fit parameters will be computed by refitting many versions of the correlation functions (e.g.\ those built with different reconstruction cuts) and comparing the results. 
This will be discussed in greater detail in Section \ref{sec:SysErrorsFitsCuts}.

\subsection{Consistency checks for uncorrelated histograms}
\label{sec:ConsistencyCheckUncorrelated}
One of the tools we employ in the search for systematic effects is the TH1::Chi2Test(TH1* h1) method, which compares two histograms bin-by-bin to determine a $\chi^2$.  
The test is performed assuming the hypothesis that the two histograms are Poissonian samples of the same underlying distribution.  
A p-value is computed which represents the probability that a measurement could be performed that would yield results this different or more, given the hypothesis of identity.  
The hypothesis of identity is rejected in instances where the p-value is lower than 0.01. 
At that threshold, less than one percent of tests will fail of data sets sampled from the same underlying distribution.  
This corresponds to rejecting the hypothesis of identity for data samples that differ by roughly $3 \sigma$.  
The 0.01 metric is somewhat arbitrary - it was chosen to be small enough that pure statistical fluctuations should not often lead to false or unnecessary systematic errors.  
For example, if a p-value of 0.1 were used instead, this would result in roughly 10\% of "good" results failing the test and contributing to the uncertainty, which would lead to a gross overestimation of the total systematic error.  
Similarly, a threshold of 0.001 would probably underestimate the error.  
Thus, 0.01 was chosen.

In this analysis, Chi2Test() will be used to compare two correlation functions which are uncorrelated with each other (uncorrelated in the sense that they are completed independent data samples).  
One example where this can be employed is the comparison of correlation functions constructed using data taken under different field configurations.  
This analysis needs all the data available to it, so results taken under different field configurations will need to be merged to improve statistics.  
But before they are combined, it must be checked that they contain compatible results.  
The Chi2Test() method provides a means of characterizing the degree of similarity of the correlation functions.  

Correlation functions that surpass the significance level of 0.01 will be treated as being functionally the same within statistics, and their weighted average will be computed without recourse to systematic uncertainty. 
In the instances where they do not exceed the significance level, they will be analyzed on a case-by-case basis.  
In particular, we will look to see if there are any systematic differences between the correlation functions.  
For example, it may be seen that the lowest ten $k^*$ bins will all be higher in one correlation function than another.  
If so, the data will still be merged (so long as the results aren't dramatically different), but a systematic uncertainty will be computed and applied to the final averaged correlation function in the form of systematic error bars.  
%Section \ref{sec:CalculatingSysErrors} describes how these uncertainties are calculated.  

When using Chi2Test() to compare correlation functions, the trouble areas that result in small p-values will likely be the lowest $k^*$ bins (which exhibit most of the interesting physics, and which also suffer the most from two-track effects), and large $k^*$ bins (close to $1 \mathrm{GeV/c}$).  
The height of the the large $k^*$ bins is noticeably susceptible to statistical (or systematic) deviations in the normalization region.  
The high $k^*$ bins have relatively small statistical error bars, so small deviations of normalization between correlation functions can result in large $\chi^2$ values.  
The correlation functions are all normalized to unity in the $0.3 < k^* < 0.5 \mathrm{GeV}/c$ range, so there are unlikely to be significant deviations seen in that range.  

It is important to emphasize that Chi2Test() is only appropriate for comparing two uncorrelated histograms.  
In the case of correlated histograms (e.g.\ correlation functions constructed using slightly different two-track cuts or different reconstruction cuts), a different method must be used to test the histograms for consistency. 

In this analysis, $\chi^2$ tests of correlation functions with different magnetic field configurations found that the results from the two fields were consistent with each other.
Therefore, the magnetic field configurations to not to contribute to the systematic uncertainties on the correlation functions or fit results. 


\subsection{Consistency checks for correlated histograms}
\label{sec:ConsistencyCheckCorrelated}
Chi2Test() cannot be used to compare correlated data because the test is performed assuming the statistical error bars on the two histograms are independent.  
That is obviously not the case for two histograms that differ in their content by only a few percent.  
In this analysis, we follow the advice of Roger Barlow \cite{Barlow:2002yb} and perform consistency checks of correlated data in the following fashion:

\begin{enumerate}
\item Take the difference between the two correlation function histograms $$\Delta C(k^*) = C_1(k^*) - C_2(k^*),$$ using $C_1\rightarrow$ Add($C_2$,$-1$).  
The resulting histogram shows the usually small differences between the two data sets.
\item Here, ROOT adds the errors in quadrature, which isn't correct for correlated data.  
To fix this, we manually set the error bars using $\sigma_{\Delta}(k^*) = \sqrt{ \abs{ \sigma_1^2(k^*) - \sigma_2^2(k^*) }}$.  
This error is correct in the specific case (found here) that one histogram is entirely a subset of the other.
\item We now look to see if there are any significant discrepancies or systematic differences between the two original correlation functions.
If there is a structural difference, it will generally be largest in the lowest $k^*$ bins, dropping off as $k^*$ increases.
This shape is reasonably described by a decaying exponential: $f(k^*) = a \exp(-b k^*)$. 
The amplitude $a$ tells us the size of the discrepancy. The width $b$ has no significance; it just helps the fit.
\item We fit the histogram with the exponential, and we extract the amplitude $a$ and its error $\sigma_a$.
For this analysis, we conclude that there is no discrepancy if $a$ is within 2$\sigma$ of zero.
If $a$ is more than 2$\sigma$ from zero, then we have a source of systematic uncertainty.
\item In the cases where there is a systematic deviation, we take the height of the fit function in the center of each $k^*$ bin as the systematic uncertainty for that bin from this cut.
As the fit function can be positive or negative, this leads to asymmetric errors.

\end{enumerate}

Using the steps listed above, we can evaluate if seemingly small changes to the cuts employed in correlation function construction change the resulting data in any significant way.  


\subsection{Systematic errors from reconstruction cuts}
\label{sec:SystematicsReconstruction}

In this section we discuss the systematic errors associated with the V0 reconstruction cuts (e.g.\ cosine of pointing angle, DCA to primary vertex, etc.).  
Loosely speaking, these systematic checks test the sensativity of the correlation function to small changes in the $\Lambda$ purity.
The optimal cut values were discussed in Section \ref{sec:Recon}.  
For example, the optimal cut value chosen for the DCA of daughter tracks to each other was 4 mm (daughters tracks were required to pass within that distance of each other).    
However, there is some ambiguity in determining the optimal cut value.  
For example, any value within $4\pm10\%$ would be reasonable.
The correlation function should ideally be insensitive to small changes in the cut value.
If a small tweak to the cut values produces a statistically significant change in the correlation function, that is a source of systematic uncertainty.

To test for systematic differences, we make correlation functions using $\Lambda$ ($\bar{\Lambda}$) reconstructed with these different cut values. 
At any given time, one cut type (e.g.\ DCA of proton daughter to primary vertex) will be varied away from the default cut value by $\pm10\%$, while the other cut types (e.g.\ cosine of pointing angle) are fixed to their default values. 
  
Data has been collected for variations of the following cuts: 
\begin{itemize}
\item DCA of proton daughter to primary vertex
\item DCA of pion daughter to primary vertex
\item DCA of daughters to each other
\item Proper decay length of $\Lambda$
\item Eta of $\Lambda$
\item Cosine of $\Lambda$ pointing angle
\item DCA of the $\Lambda$ to primary vertex
\item $p_{\mathrm{T}}$ of $\Lambda$
\item Reconstructed mass ($m_{\pi\mathrm{p}}$)
\end{itemize}
  
In each case, correlation functions have been constructed for each pair type ($\Lambda\Lambda$, $\bar{\Lambda}\bar{\Lambda}$, and $\Lambda\bar{\Lambda}$), and for the three cut values (optimal cut, cut + 10\%, cut - 10\%).  
The optimal cut correlation functions are compared with the tighter and looser cut correlation functions.  
The correlation functions all share much of the same data, so they are highly correlated with each other.  
We therefore analyze the changes in the correlation function using the method described in Section \ref{sec:ConsistencyCheckCorrelated}.

Checking each cut yields a positive, negative, or non-existent systematic uncertainty from that cut for each $k^*$ bin. 
For a given bin, we take the positive and negative values with the largest magnitude to be the upper and lower asymmetric errors, respectively.


\subsection{Systematic errors from pair-wise cuts}
\label{sec:SystematicsPairWise}

The systematic uncertainties from the average separation cut are implemented in exactly the same way as the single-particle reconstruction cuts in the previous section. 
For each cut value, we vary the cut by $\pm10\%$, construct correlation functions, and find the uncertainty for each $k^*$ bin via the method in Section \ref{sec:ConsistencyCheckCorrelated}.
In general, the systematic uncertainties are quite small --- about an order of magnitude smaller than the uncertainties from the reconstruction cuts.

\subsection{Systematic errors from shared-daughter cut}

At this time, two femtoscopic analyses at ALICE employ a shared-daughter cut (this analysis and the $\mathrm{K}^0_\mathrm{S}$ analysis), though each cut is implemented with independent code.  
The reasons for this cut are intuitive - two V0s cannot actually have the same daughter, regardless of what might arise from the reconstruction process.  
Moreover, the efficacy of the cut is high: the MC studies of this cut have shown that only 13\% real V0s with shared daughters are cut.  
The removal of a V0 (real or fake) removes all of the same-event and mixed-event pairs that would have included that V0.  
In most cases, this amounts to an improved purity of the correlation function, since there are fewer pairs of fake and therefore uncorrelated particles.  
There also a removal of a splitting-like effect - in this case the "split" is an extra V0 close in momentum space to the first.  
While the two "split" V0s will never be paired together, both are paired with other V0s, resulting in extra pair counts in roughly the same $k^*$ bins.  
Those extra pairs are removed by virtue of the shared daughter cut.

While the benefits of the shared-daughter cut are apparent,
and the disadvantages seem to be minimal (even when you cut a real V0, you a left with a fake V0 that is close in phase space), it is unclear at this time how to determine what systematic uncertainties may arise from this cut and how to quantify that uncertainty.

\subsection{Combining different sources of systematic error}
\label{sec:CombiningSys}

Systematic errors have been evaluated for reconstruction cuts and average separation cuts.
The reconstruction cuts and the separation cuts both yield asymmetric errors for each $k^*$ bin of the correlation function.
We then add those asymmetric errors in quadrature (positive with positive, negative with negative) to get the full systematic uncertainty on the correlation functions.
The resulting correlation functions with combined systematic errors are shown in Section \ref{sec:CorrelationFunctionResults}.
For all correlation functions and all $k^*$ bins, the systematic uncertainties are much smaller than the statistical error bars.

